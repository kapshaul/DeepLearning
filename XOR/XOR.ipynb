{"cells":[{"cell_type":"markdown","metadata":{"id":"3grE2-eO1zSV"},"source":["# Solving XOR Problem with Deep Learning\n","\n","One of the most prominent limitations of a linear model is its inability to solve the XOR problem. While a linear model can handle AND and OR operations, it fails with the XOR operation. In the study of AI, it is commonly known that a Multilayer Perceptron (MLP) can solve the XOR problem. This document explores how this is achieved.\n","\n","## 1. Limitations of a Linear Model\n","\n","When classifying data, a linear model can only create a linear boundary, effectively dividing data into two categories using a straight line. This approach works for operations like AND and OR, where a linear line can separate the outputs (0 and 1). However, the XOR operation is not linearly separable, meaning a single straight line cannot separate the outputs for XOR.\n","\n","This limitation of the perceptron was mathematically demonstrated by Marvin Minsky and Seymour Papert in their book *Perceptrons: An Introduction to Computational Geometry* (1969).\n","\n","## 2. Solving XOR with a Hidden Layer\n","\n","To solve the XOR problem, at least two lines or a non-linear boundary are required. In this document, we demonstrate how two lines can be used to solve the XOR problem.\n","\n","### XOR Logic\n","\n","Conceptually, the XOR operator can be represented by the intersection of two logical operations:\n","\n","1. **NOT (x1 AND x2)** - A line representing this operation.\n","2. **(x1 OR x2)** - A line representing this operation.\n","\n","The intersection of these two lines (AND operation) yields a result of 1 where the XOR condition is met, and 0 otherwise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PJc2tI9fElm"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1038,"status":"ok","timestamp":1724593531445,"user":{"displayName":"Yong","userId":"14720898434364085220"},"user_tz":-540},"id":"I1sgHGO1fVWQ"},"outputs":[],"source":["# XOR input output data\n","x = [\n","     [0, 0],\n","     [0, 1],\n","     [1, 0],\n","     [1, 1]\n","] * 5000\n","\n","y = [\n","     [0],\n","     [1],\n","     [1],\n","     [0]\n","] * 5000\n","\n","x = np.array(x)\n","y = np.array(y)"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":972,"status":"ok","timestamp":1724594318094,"user":{"displayName":"Yong","userId":"14720898434364085220"},"user_tz":-540},"id":"O9ztp66S0zi7"},"outputs":[],"source":["def print_weights(model, state):\n","    print()\n","    print(f\"Model weights {state}:\")\n","    weights = model.get_weights()[0]\n","    for i in range(len(weights)):\n","        print(f\"{i} layer weights = {weights[i]}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"LkGnZkZZBtyp"},"source":["## 3. Classification of XOR with MLP\n","\n","### Model Architecture\n","\n","To classify XOR, the input layer must consist of neurons corresponding to the input pairs (0,0), (0,1), (1,0), and (1,1). Therefore, the input layer contains two neurons. The hidden layer needs at least two neurons to form the two lines necessary for classification. These neurons are fully connected (FC) and use weights, biases, and the Sigmoid activation function.\n","\n","### Activation Function\n","\n","The Sigmoid function is used as the activation function. It approaches 1 as the input becomes more positive and 0 as the input becomes more negative.\n","\n","```bash\n","# Define the neural network model\n","model = tf.keras.Sequential([\n","    # Fully connected layer with sigmoid activation\n","    tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(2,)),\n","    \n","    # Output layer with sigmoid activation\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","```"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":812},"executionInfo":{"elapsed":7898,"status":"ok","timestamp":1724594326520,"user":{"displayName":"Yong","userId":"14720898434364085220"},"user_tz":-540},"id":"WTpwWysUfVY9","outputId":"43ac7d31-59d1-4aa1-e76f-1dd26a27f197"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_11\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │               \u001b[38;5;34m6\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m3\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> (36.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9\u001b[0m (36.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> (36.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9\u001b[0m (36.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Model weights before:\n","0 layer weights = [-0.99807197 -0.00325847]\n","1 layer weights = [-1.0873995  0.8312775]\n","\n","Epoch 1/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6718 - loss: 0.5083\n","Epoch 2/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0084\n","Epoch 3/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0029\n","Epoch 4/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0015\n","Epoch 5/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 9.6297e-04\n","Epoch 6/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.6401e-04\n","Epoch 7/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.7684e-04\n","Epoch 8/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.5842e-04\n","Epoch 9/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.7687e-04\n","Epoch 10/10\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.1737e-04\n","\n","Model weights after:\n","0 layer weights = [-12.238248  -9.635029]\n","1 layer weights = [-12.6187725  -9.6489105]\n","\n"]}],"source":["# Define the neural network model\n","model = tf.keras.Sequential([\n","    # Fully connected layer with sigmoid activation\n","    tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(2,)),\n","\n","    # Output layer with sigmoid activation\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.summary()\n","\n","# Model weights before training\n","print_weights(model, \"before\")\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n","model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n","\n","model.fit(x, y, batch_size=64, epochs=10)\n","\n","# Model weights after training\n","print_weights(model, \"after\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQ5BlWvG0qbR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
